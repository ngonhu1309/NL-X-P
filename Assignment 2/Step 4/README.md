# Parameter Comparison Analysis

To understand the effects of embedding dimensionality, retrieval depth, and prompting strategies on Retrieval-Augmented Generation (RAG) performance, we conducted a systematic series of nine experiments for two embedding models: `all-MiniLM-L6-v2` (384 dimensions) and `all-mpnet-base-v2` (768 dimensions). Each embedding model was paired with three retrieval depths (top-1, top-3, top-5) and three prompting strategies (Instructional, Chain-of-Thought, and Persona). Performance was measured using Exact Match (EM) and F1 Score, following standard QA evaluation methodology.

### Embedding Size Comparison (384D vs 768D).
Results indicate that increasing embedding dimensionality from `384D (MiniLM)` to `768D (MPNet)` did not uniformly improve accuracy. For **Instructional prompting**, `MiniLM (384D)` slightly outperformed `MPNet (768D)` in the top-1 and top-3 retrieval settings, with EM scores of 34.64 and 32.46 versus `MPNet`’s 33.99 and 31.80, respectively. Similarly, F1 scores were nearly identical across models (≈42 for top-1). This suggests that while higher-dimensional embeddings offer richer semantic representations, the marginal benefits are small for a compact dataset like Mini Wikipedia. Moreover, the increased computational overhead of 768D embeddings may not justify the negligible performance gains, especially in real-time systems.

### Retrieval Depth Comparison (Top-1 vs Top-3 vs Top-5).
Expanding retrieval depth showed mixed results. For **Instructional prompting**, performance peaked at top-1 or top-3 and declined at top-5. For example, in `MiniLM` runs, F1 dropped from 42.13 (top-1) to 30.09 (top-5). Similarly, `MPNet`’s F1 declined from 41.67 (top-1) to 27.86 (top-5). This pattern indicates that additional retrieved contexts may introduce irrelevant or noisy passages, diluting answer accuracy rather than improving it. In contrast, Persona prompting followed a similar downward trend, with F1 falling from ~34 (top-1) to ~25 (top-5). These results reinforce that more retrieval is not always better, and optimal depth depends on balancing recall with precision in the context set.

### Prompting Strategy Comparison.
Prompting strategy emerged as the most influential factor. Instructional prompting consistently outperformed both Persona and Chain-of-Thought across all dimensions and depths. For instance, **Instructional prompting** with `MiniLM top-1` achieved the highest F1 score (42.13), while **Persona** lagged (F1 = 34.09) and **Chain-of-Thought** collapsed (F1 = 10.22). This trend was replicated in MPNet runs, with CoT again failing to produce reliable results (F1 ≈ 7-10). The poor performance of CoT can be attributed to over-reasoning. Instead of focusing on concise factual answers, the model generated verbose or speculative outputs that did not align with ground-truth answers, leading to near-zero Exact Match. Persona prompting, while capturing a conversational style, also underperformed compared to Instructional, likely because stylistic framing diluted factual precision. Therefore, Instructional prompting strikes the best balance between grounding and directness in RAG contexts.

### Key Takeaways
Three conclusions emerge from this comparison. First, embedding size (384D vs 768D) does not significantly alter outcomes on smaller QA datasets, suggesting that lower-dimensional embeddings are computationally efficient without sacrificing accuracy. Second, retrieval depth must be tuned carefully: top-1 or top-3 yields optimal performance, while top-5 introduces harmful noise. Third, prompting style has the strongest impact, with Instructional prompting outperforming alternatives by a wide margin.
