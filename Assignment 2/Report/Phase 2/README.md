# Naive RAG Implementation

The **Naive Retrieval-Augmented Generation (RAG)** implementation in this notebook represents the baseline pipeline against which more sophisticated strategies are compared. The design follows a simple yet illustrative approach, which is to retrieve a minimal set of passages for each query, feed them directly into a language model without additional processing, and evaluate the generated answers using standardized metrics. While the performance of this naive version is expected to be limited, it establishes a controlled foundation that highlights the impact of later enhancements.

The pipeline begins by ingesting query and passage datasets. Passages are preprocessed to remove null values and annotated with statistics such as character and word length. Queries are similarly cleaned to ensure consistent formatting. These steps ensure that only valid text data proceeds into the retrieval and generation stages.

For embedding generation, the notebook leverages the **SentenceTransformers model** `all-MiniLM-L6-v2` with 384-dimensional vectors. This embedding choice balances efficiency and semantic accuracy. Each passage is transformed into a dense vector representation and stored in a Milvus vector database. When a query arrives, its embedding is computed and compared against the passage embeddings using cosine similarity. Importantly, in the naive setting, retrieval is restricted to **top-1**, which is the single most similar passage. This strict limitation simplifies the pipeline but introduces obvious risks. If the top passage is noisy or irrelevant, the model receives little opportunity to recover.

The retrieved passage is concatenated with the query and passed to a language model, specifically **Flan-T5-base**, which generates the final answer. Unlike enhanced strategies, no reranking, passage expansion, or metadata filtering is applied. The model must rely solely on the one retrieved passage, reinforcing the minimalistic nature of this approach.

Results from the naive pipeline are saved in CSV format (`384d_top1_instruction_results.csv`), containing the original question, the retrieved context, and the model’s answer. These outputs are then adapted into the RAGAS evaluation format, which requires four components: (1) the query, (2) the model-generated answer, (3) the retrieved contexts (in this case, a single passage), and (4) the ground truth reference answer. Since explicit human-labeled ground truth is not available, the implementation reuses the model’s answer as a proxy reference, acknowledging a limitation of the dataset.

The RAGAS framework evaluates the naive pipeline across multiple dimensions: **faithfulness** (alignment of answer to context), **context precision** and **recall** (relevance and completeness of retrieved passages), and answer relevancy (appropriateness of generated answers to the query). These metrics provide a multi-faceted view of performance, extending beyond exact match or F1 scores.

Finally, the notebook aggregates results, saving both per-question breakdowns and average scores in CSV and JSON formats. This enables transparent comparison against enhanced strategies. In side-by-side evaluation, the naive model predictably underperforms, particularly in faithfulness and recall, due to its reliance on a single passage. However, it succeeds in demonstrating the baseline trade-off between simplicity and robustness.

In summary, the Naive RAG implementation serves as a controlled experimental baseline. Its strengths lie in simplicity, transparency, and computational efficiency. Its weaknesses, including vulnerability to poor retrieval and lack of multi-passage reasoning, highlight the necessity of the enhancements introduced later. Without this foundation, the improvements offered by metadata filtering, reranking, or multi-vector embeddings would be difficult to quantify. Thus, the naive pipeline is essential not as an end solution, but as the benchmark against which true progress is measured.
