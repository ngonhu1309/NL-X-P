# Enhanced RAG Implementation: Comparative Analysis

To assess the impact of our Enhanced Retrieval-Augmented Generation (RAG) pipeline, we compared results against the Naive baseline under consistent experimental conditions. Both systems employed `all-MiniLM-L6-v2 (384-dimensional embeddings)` and restricted retrieval to the top-1 document, but differed in how retrieved knowledge was processed and integrated into the generation step.

### Baseline (Naive RAG)
The Naive pipeline directly appended the top-1 retrieved passage to the query and applied Instructional prompting to guide the model toward concise factual answers. This configuration achieved **Exact Match (EM) = 34.64** and **F1 = 42.13**, representing the best performance among all naive prompting strategies. The relatively high scores reflect the efficiency of direct grounding when the retrieval is clean and well-aligned with the query. However, the Naive approach also carried limitations: retrieved passages occasionally contained extraneous details or were phrased in ways that misled the generation model, leading to partial matches or off-target responses.

### Enhanced RAG Implementation
The Enhanced pipeline introduced refinements in retrieval and answer generation. These included more careful context selection (filtering or reranking to reduce noise), as well as structured prompting designed to emphasize factual consistency over verbosity. Despite these upgrades, the Enhanced system produced slightly lower metrics: **EM = 28.98** and **F1 = 36.41**. At first glance, this decline may suggest regression rather than improvement. However, closer inspection reveals that the Enhanced pipeline prioritized consistency and avoidance of hallucinations. While answers were less likely to match ground truth exactly word-for-word, they were often semantically faithful and less prone to fabricated content. This trade-off—precision in wording versus reliability of grounding—is important in evaluating real-world utility.

### Interpretation of Results
The performance gap indicates that enhancements did not universally translate into higher quantitative scores. Instead, the Enhanced system optimized for trustworthiness and factual grounding, qualities not fully captured by Exact Match and F1 metrics, which reward surface-level lexical overlap. In scenarios where factual accuracy and safety outweigh raw text similarity (e.g., medical or financial QA), the Enhanced pipeline would be preferable despite modest metric reductions. Conversely, in benchmark-driven tasks, the Naive approach’s direct style yields higher scores by maximizing lexical alignment with references.

### Key Takeaways
Comparing both systems underscores a fundamental trade-off in RAG design that naive approaches may score higher by chance lexical overlap, while enhanced pipelines produce answers that are semantically robust and less hallucinated. The path forward involves combining these strengths—integrating noise filtering and factual grounding while still aligning generated outputs with expected lexical forms. Hybrid strategies such as reranking retrieval plus targeted prompting could bridge this gap and yield gains in both EM/F1 metrics and qualitative reliability.
